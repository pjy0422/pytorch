{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vex99np2wFVt"
      },
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/). \n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA). \n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "17dd5453-9639-4b01-aa18-7ddbfd5c3253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Mar 13 14:38:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.89       Driver Version: 465.89       CUDA Version: 11.3     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:2D:00.0 Off |                  N/A |\n",
            "| N/A   55C    P8    N/A /  N/A |    119MiB /  2048MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "9c150c50-a092-4f34-9d33-b45247fb080d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.12.1\n"
          ]
        }
      ],
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSFX7tc1w-en"
      },
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyWRkvWGbCXj"
      },
      "source": [
        "1. Autonomous driving\n",
        "2. Factory anomaly detection\n",
        "3. China CCTV surveilance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBK-WI6YxDYa"
      },
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d1rxD6GObCqh"
      },
      "source": [
        "Overfitting means the model is trained on training set `too` well ,but the performance on the test set and generalization power are relatively poor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeYFEqw8xK26"
      },
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. \n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ocvOdWKcbEKr"
      },
      "source": [
        "1. Regularization\n",
        "2. Cross validation\n",
        "3. Early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKdEEFEqxM-8"
      },
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvf-3pODxXYI"
      },
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SHjeuN81bHza"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_data = datasets.MNIST(train=True, transform=ToTensor(), target_transform=None,  download=True, root = './data/')\n",
        "test_data = datasets.MNIST(train=False, transform=ToTensor(), target_transform=None,  download=True, root='./data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZW-uAbxe_F"
      },
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QVFsYi1PbItE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  plt.subplot(1,5,i+1)\n",
        "  img, label = train_data[i]\n",
        "  plt.imshow(img.squeeze(), cmap='gray')\n",
        "  plt.title(train_data.classes[label])\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPDzW0wxhi3"
      },
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ALA6MPcFbJXQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(dataset = train_data, batch_size = 32)\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=32)\n",
        "train_classes = train_data.classes\n",
        "test_classes = test_data.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCVfXk5xjYS"
      },
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self,input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.conv_layer1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv_layer2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(in_features=49*hidden_units, out_features=output_shape)\n",
        "    )\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    #print(x.shape)\n",
        "    x = self.conv_layer1(x)\n",
        "    #print(x.shape)\n",
        "    x = self.conv_layer2(x)\n",
        "    #print(x.shape)\n",
        "    #print(x.flatten().shape)\n",
        "    x = self.classifier(x)\n",
        "    #print(x.shape)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0127,  0.0260,  0.0440,  0.0179, -0.0193, -0.0362,  0.0303,  0.0112,\n",
              "         -0.0350, -0.0346]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_2 = TinyVGG(input_shape=1, hidden_units=10, output_shape=len(train_classes))\n",
        "img,label = train_data[0]\n",
        "img.unsqueeze(dim=0).shape\n",
        "model_2(img.unsqueeze(dim=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_3zUr7xlhy"
      },
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup essential functions and metrics\n",
        "from helper_functions import accuracy_fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params= model_2.parameters(), lr = 0.1)\n",
        "accuracy_fn = accuracy_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\clack\\pytorch\\03_pytorch_vision_exercises.ipynb Cell 24\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model_2\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m X,y \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   train_logit \u001b[39m=\u001b[39m model_2(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   train_pred \u001b[39m=\u001b[39m train_logit\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   train_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39maccuracy_fn(y_true\u001b[39m=\u001b[39my, y_pred \u001b[39m=\u001b[39m train_pred)\n",
            "File \u001b[1;32mc:\\Users\\clack\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\clack\\pytorch\\03_pytorch_vision_exercises.ipynb Cell 24\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x:torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   \u001b[39m#print(x.shape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_layer1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   \u001b[39m#print(x.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clack/pytorch/03_pytorch_vision_exercises.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layer2(x)\n",
            "File \u001b[1;32mc:\\Users\\clack\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\clack\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\clack\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\clack\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[1;32mc:\\Users\\clack\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from timeit import Timer as timer\n",
        "\n",
        "train_start_cpu = timer()\n",
        "\n",
        "epochs = 5\n",
        "device = 'cpu'\n",
        "train_acc, train_loss = 0,0\n",
        "for epoch in range(epochs):\n",
        "  model_2.train()\n",
        "  for X,y in train_dataloader:\n",
        "    train_logit = model_2(X)\n",
        "    train_pred = train_logit.softmax(dim=0).argmax(dim=1)\n",
        "    train_acc +=accuracy_fn(y_true=y, y_pred = train_pred)\n",
        "    loss = loss_fn(train_logit, y)\n",
        "    train_loss+=loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  train_loss /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "  print(f\"train loss : {train_loss:.5f} | train_acc : {train_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1CsHhPpxp1w"
      },
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQwzqlBWxrpG"
      },
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj6bDhoWxt2y"
      },
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHS20cNTxwSi"
      },
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset. \n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been. \n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error? \n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMUsDcN/+FAm9Pf7Ifqs6AZ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "03_pytorch_computer_vision_exercises.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
